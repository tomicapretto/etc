{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note about how evident is the difference between Bambi models, and it's not so evident in PyMC sometimes.\n",
    "\n",
    "* `y ~ x:group` vs `y ~ x`\n",
    "    * The difference between those models written in PyMC is very subtly. In Bambi it's more evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 60: Understanding encodings\n",
    "\n",
    "* Categorical variables need to be encoded into numbers somehow\n",
    "* A default approach is to create dummy variables\n",
    "* But since the sum(x, axis=1) adds up to a column of 1s, keeping all the dummies would introduce perfect association between predictors if we also have an intercept\n",
    "    * And for complicated reasons this is not something we want. \n",
    "    * It's going to break our inferences\n",
    "    * We say the model is not-identified\n",
    "* Different alternatives\n",
    "    * Never use an intercept\n",
    "        * It's successful only if we have one categorical predictor\n",
    "        * The meaning of the coefficient is the mean of the group\n",
    "            * This is known as cell-means encoding\n",
    "    * Use encodings that apply restrictions\n",
    "        * By default, Bambi uses Treatment encoding (or Reference encoding)\n",
    "        * One level is dropped, and that level is the Reference level of the variable\n",
    "        * The meaning of the coefficients is then the difference between the any category and the reference level\n",
    "* It's a complex topic\n",
    "    * There are many different encodingss\n",
    "    * And things get more complicated when there are more categorical terms, it's hard to make sure we're not introducing non-identifiabilities\n",
    "    * Fortunately, Bambi handles all these cases automatically for us\n",
    "    * Even though it's not completely clear for us, we can rely on Bambi :)\n",
    "    * But it's always good to try to figure out what's going on!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Priors in Bambi\n",
    "\n",
    "* Automatic priors\n",
    "* How to change priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts and things I don't want to miss\n",
    "\n",
    "* Bambi\n",
    "* Automatic Priors\n",
    "* Formulae syntax\n",
    "* Prior predictive sampling in Bambi\n",
    "* Predictions in Bambi\n",
    "\n",
    "...\n",
    "\n",
    "* Bambi has automatic priors.\n",
    "  * Main points:\n",
    "    * Designed to provide low to moderate shrinkage/regularization in most situations.\n",
    "    * Allows faster prototyping and iteration.\n",
    "    * Helps you getting focused on the model and the problem. You can refine priors once your model is selected.\n",
    "* Bambi allows to pass inline transforms (eg log(y) ~ x).\n",
    "    * Compare manual transform with inline transform.\n",
    "* Show Bambi repr and how nice it summarizes everything\n",
    "\n",
    "... \n",
    "\n",
    "* At the end of this lesson this person should feel impressed with Bambi and understand\n",
    "  * The equivalent functionaly\n",
    "  * Syntatical sugar of Bambi\n",
    "  * Less code to type, fewer errors to make.\n",
    "  * Bambi allows to iterate faster when exploring different models.\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "* Bambi vs PyMC comparison (in terms of code)\n",
    "* When do we talk about power terms?\n",
    "    * They will be useful for the concrete example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Section 20\n",
    "\n",
    "* Reproduce the plot that visualizes the line fitted by the model, using the Bambi model.\n",
    "    * You don't need to write a lot of new code. \n",
    "    * Use the snippet from Lesson 2, find the appropriate places where you need to replace parameter names\n",
    "    * You're ready to go!\n",
    "\n",
    "This is the snippet. Most of it customizes the visualization. Find what's needed to be replaced, and replace it to generate the visualization using the output of the Bambi model.\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.scatter(x=data[\"Length1\"], y=data[\"Weight\"], alpha=0.6)\n",
    "ax.set(xlabel=\"Length (centimeters)\", ylabel=\"Weight (grams)\", title=\"Fish length vs weight\");\n",
    "\n",
    "for value in idata.posterior[\"β0\"].to_numpy().flatten()[::10]:\n",
    "    ax.axhline(value, color=\"C1\", alpha=0.2)\n",
    "\n",
    "β0_mean = idata.posterior[\"β0\"].to_numpy().mean()\n",
    "ax.axhline(β0_mean, color=\"C4\")\n",
    "\n",
    "handles = [\n",
    "    Line2D([], [], label=\"Observations\", lw=0, marker=\"o\", color=\"C0\", alpha=0.6),\n",
    "    Line2D([], [], label=\"Posterior draws\", lw=2, color=\"C1\"),\n",
    "    Line2D([], [], label=\"Posterior mean\", lw=2, color=\"C4\"),\n",
    "]\n",
    "ax.legend(handles=handles, loc=\"upper left\");\n",
    "```\n",
    "\n",
    "### Section 50\n",
    "\n",
    "The model is `model = bmb.Model(\"log(Weight) ~ 1 + log(Length1)\", data)`\n",
    "\n",
    "* Use `plot_cap()` with this model. \n",
    "    * What do you see?\n",
    "    * What's the scale of each variable?\n",
    "    * Why?\n",
    "    * How can you put everything on the transformed scale?\n",
    "    * And everything on the untransformed scale again?\n",
    "* Ask students to use advanced transformations\n",
    "* What happens if you do \n",
    "    * x / 100 or x ** 2?\n",
    "    * What if you do `x/y`? and `(x + y) ** 2`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 80\n",
    "\n",
    "* Repeat this experiment with different sample sizes.\n",
    "    * Use $n=500$ and $n=2000$\n",
    "    * Explore the posterior of the slopes\n",
    "    * Change the seeds and comment on the posterior results\n",
    "    * Do you get any warnings?\n",
    "        * Explore ESS and R-hat\n",
    "        * Explore the correlation between $b_1$ and $b_2$. Does it improve with more data?\n",
    "* Now, implement the model with a single slope.\n",
    "    * Use $n=500$ and $n=2000$\n",
    "    * Compare the sampling speed with the previous model\n",
    "        * What can you conclude about the effects of parameter correlations in sampling speed?\n",
    "    * Can you recover the true slope?\n",
    "    * Do you get any warnings? Again explore ESS and R-hat.\n",
    "    * Also explore the correlation between model parameters. What can you conclude?\n",
    "* Try to get the flawed model in Bambi\n",
    "    * Hint: Use the formula `\"y ~ x + x\"`. \n",
    "    * What happens? Why do you think it works that way?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 110\n",
    "\n",
    "* Fit the model using other numerical predictors, compute the $R^2$, and compare to what we got here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ib_advanced_regression')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96b52e1d985b6c457db8356caeb4e1b2f8f20b187e2d2aa9e312ebd461d4cda7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
