{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Make some art\n",
    "\n",
    "In the section \"The world's simplest model\" from \"Lesson 2: Regression Refresher\" we created the following visualization.\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/intercept_only_model_curve.png\" style=\"width:950px;\"/>\n",
    "</center>\n",
    "\n",
    "This allowed us to see the intercept-only model fits a flat line that is not affected by the values of the predictor. \n",
    "\n",
    "You're asked to reproduce this visualization, usin the Bambi model we created in this lesson. Fortunately, you don't need to write a lot of code from scratch. Use the snippet from Lesson 2 and find the places where you need to change parameter names. \n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "ax.scatter(x=data[\"Length1\"], y=data[\"Weight\"], alpha=0.6)\n",
    "ax.set(xlabel=\"Length (centimeters)\", ylabel=\"Weight (grams)\", title=\"Fish length vs weight\");\n",
    "\n",
    "for value in idata.posterior[\"β0\"].to_numpy().flatten()[::10]:\n",
    "    ax.axhline(value, color=\"C1\", alpha=0.2)\n",
    "\n",
    "β0_mean = idata.posterior[\"β0\"].to_numpy().mean()\n",
    "ax.axhline(β0_mean, color=\"C4\")\n",
    "\n",
    "handles = [\n",
    "    Line2D([], [], label=\"Observations\", lw=0, marker=\"o\", color=\"C0\", alpha=0.6),\n",
    "    Line2D([], [], label=\"Posterior draws\", lw=2, color=\"C1\"),\n",
    "    Line2D([], [], label=\"Posterior mean\", lw=2, color=\"C4\"),\n",
    "]\n",
    "ax.legend(handles=handles, loc=\"upper left\");\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Deepen your `plot_cap()` knowledge\n",
    "\n",
    "In this exercise you have to work with the fish data and the model we created in the \"Transformations in Bambi\" section. More concrete, you need to build the following model\n",
    "\n",
    "```python\n",
    "`model = bmb.Model(\"log(Weight) ~ 1 + log(Length1)\", data)`\n",
    "```\n",
    "Notice you need to import the data to solve this exercise. Then create and fit the model mentioned above, and use `plot_cap()` with `Length1` as the variable for the horizontal axis.\n",
    "\n",
    "```python\n",
    "plot_cap(model, idata, \"Length1\")\n",
    "```\n",
    "\n",
    "Answer the following questions\n",
    "\n",
    "* What do you see?\n",
    "* What is the scale of the variables in the horizontal and the vertical axis? Consider they're used with an inline transformation in the model.\n",
    "    * Why?\n",
    "* How can you put both variables in the transformed scale?\n",
    "* And how you put everything on the untransformed scale?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Advanced operators\n",
    "\n",
    "In the section \"Transformations in Bambi\" we showed that some custom operations needed to be written using a particular syntax. In this exercise you're going to experiment a little with with these operations to see how they work in practice. The goal here is not to arrive to a \"correct\" answer, but to see what are the different results when we try different formulas and trying to understand what's going on. Use the following simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1211)\n",
    "size = 100\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"response\": rng.normal(loc=5, scale=0.5, size=size),\n",
    "        \"x\": rng.normal(loc=10, scale=2, size=size),\n",
    "        \"y\": rng.normal(size=size),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following formulas and explain what's happening\n",
    "\n",
    "* `\"response ~ x / 100`\"\n",
    "* `\"response ~ x ** 100\"`\n",
    "* `\"response ~ x / y\"`\n",
    "* `\"response ~ (x + y) ** 2\"`\n",
    "\n",
    "It's fine if you can only describe what you see. At this point you're not expected to be an expert in formula notation. We'll cover more advanced stuff later in the course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between `\"response ~ x + y\"` and `\"response ~ I(x + y)\"`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Experiment!\n",
    "\n",
    "In the \"Parameter identifiability\" section we simulated some data to grasp what non-identifiability means using a controlled scenario. The code we used is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "b0, b1 = 0.5, 2.5\n",
    "x = np.linspace(0, 3, num=50)\n",
    "noise = rng.normal(scale=0.5, size=50)\n",
    "y = b0 + b1 * x + noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we created the following PyMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as weird_model:\n",
    "    b0 = pm.Normal(\"b0\", sigma=0.5)\n",
    "    b1 = pm.Normal(\"b1\")\n",
    "    b2 = pm.Normal(\"b2\")\n",
    "    mu = b0 + b1 * x + b2 * x\n",
    "    sigma = pm.HalfNormal(\"sigma\" ,sigma=0.5)\n",
    "    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to have you run several experiments to understand how parameter non-identifiablity affects sampling times and the quality of the draws obtained. You are asked to\n",
    "\n",
    "* Repeat this experiment with different sample sizes.\n",
    "    * Use $n=500$ and $n=2000$\n",
    "* Explore the posterior of the slopes\n",
    "* Change the seeds and comment on the posterior results\n",
    "* Do you get any warnings?\n",
    "    * Explore ESS and R-hat\n",
    "    * Explore the correlation between $b_1$ and $b_2$. \n",
    "        * Does it improve with more data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the correct model, with a single slope\n",
    "\n",
    "```python\n",
    "with pm.Model() as good_model:\n",
    "    b0 = pm.Normal(\"b0\", sigma=0.5)\n",
    "    b1 = pm.Normal(\"b1\")\n",
    "    mu = b0 + b1 * x\n",
    "    sigma = pm.HalfNormal(\"sigma\" ,sigma=0.5)\n",
    "    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit it using $n=500$ and $n=2000$, as done previously\n",
    "* Compare the sampling speed with the previous model\n",
    "    * What can you conclude about the effects of parameter correlations in sampling speed?\n",
    "* Can you recover the true slope?\n",
    "* Do you get any warnings? Again explore ESS and R-hat.\n",
    "* Also explore the correlation between model parameters. \n",
    "    What can you conclude?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you're asked to reproduce the flawed model in Bambi. To do so, you need to use the formula `\"y ~ x + x\"`. \n",
    "\n",
    "* What happens?\n",
    "* Why do you think it works that way?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "* Fit the model using other numerical predictors, compute the $R^2$, and compare to what we got here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some real world use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ib_advanced_regression",
   "language": "python",
   "name": "ib_advanced_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96b52e1d985b6c457db8356caeb4e1b2f8f20b187e2d2aa9e312ebd461d4cda7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
