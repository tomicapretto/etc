{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note about how evident is the difference between Bambi models, and it's not so evident in PyMC sometimes.\n",
    "\n",
    "* `y ~ x:group` vs `y ~ x`\n",
    "    * The difference between those models written in PyMC is very subtly. In Bambi it's more evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 60: Understanding encodings\n",
    "\n",
    "* Categorical variables need to be encoded into numbers somehow\n",
    "* A default approach is to create dummy variables\n",
    "* But since the sum(x, axis=1) adds up to a column of 1s, keeping all the dummies would introduce perfect association between predictors if we also have an intercept\n",
    "    * And for complicated reasons this is not something we want. \n",
    "    * It's going to break our inferences\n",
    "    * We say the model is not-identified\n",
    "* Different alternatives\n",
    "    * Never use an intercept\n",
    "        * It's successful only if we have one categorical predictor\n",
    "        * The meaning of the coefficient is the mean of the group\n",
    "            * This is known as cell-means encoding\n",
    "    * Use encodings that apply restrictions\n",
    "        * By default, Bambi uses Treatment encoding (or Reference encoding)\n",
    "        * One level is dropped, and that level is the Reference level of the variable\n",
    "        * The meaning of the coefficients is then the difference between the any category and the reference level\n",
    "* It's a complex topic\n",
    "    * There are many different encodingss\n",
    "    * And things get more complicated when there are more categorical terms, it's hard to make sure we're not introducing non-identifiabilities\n",
    "    * Fortunately, Bambi handles all these cases automatically for us\n",
    "    * Even though it's not completely clear for us, we can rely on Bambi :)\n",
    "    * But it's always good to try to figure out what's going on!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Priors in Bambi\n",
    "\n",
    "* Automatic priors\n",
    "* How to change priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts and things I don't want to miss\n",
    "\n",
    "* Bambi\n",
    "* Automatic Priors\n",
    "* Formulae syntax\n",
    "* Prior predictive sampling in Bambi\n",
    "* Predictions in Bambi\n",
    "\n",
    "...\n",
    "\n",
    "* Bambi has automatic priors.\n",
    "  * Main points:\n",
    "    * Designed to provide low to moderate shrinkage/regularization in most situations.\n",
    "    * Allows faster prototyping and iteration.\n",
    "    * Helps you getting focused on the model and the problem. You can refine priors once your model is selected.\n",
    "* Bambi allows to pass inline transforms (eg log(y) ~ x).\n",
    "    * Compare manual transform with inline transform.\n",
    "* Show Bambi repr and how nice it summarizes everything\n",
    "\n",
    "... \n",
    "\n",
    "* At the end of this lesson this person should feel impressed with Bambi and understand\n",
    "  * The equivalent functionaly\n",
    "  * Syntatical sugar of Bambi\n",
    "  * Less code to type, fewer errors to make.\n",
    "  * Bambi allows to iterate faster when exploring different models.\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "* Bambi vs PyMC comparison (in terms of code)\n",
    "* When do we talk about power terms?\n",
    "    * They will be useful for the concrete example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Section 50\n",
    "\n",
    "The model is `model = bmb.Model(\"log(Weight) ~ 1 + log(Length1)\", data)`\n",
    "\n",
    "* Use `plot_cap()` with this model. \n",
    "    * What do you see?\n",
    "    * What's the scale of each variable?\n",
    "    * Why?\n",
    "    * How can you put everything on the transformed scale?\n",
    "    * And everything on the untransformed scale again?\n",
    "* Ask students to use advanced transformations\n",
    "* What happens if you do \n",
    "    * x / 100 or x ** 2?\n",
    "    * What if you do `x/y`? and `(x + y) ** 2`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 80\n",
    "\n",
    "* Repeat this experiment with different sample sizes.\n",
    "    * Use $n=500$ and $n=2000$\n",
    "    * Explore the posterior of the slopes\n",
    "    * Change the seeds and comment on the posterior results\n",
    "    * Do you get any warnings?\n",
    "        * Explore ESS and R-hat\n",
    "        * Explore the correlation between $b_1$ and $b_2$. Does it improve with more data?\n",
    "* Now, implement the model with a single slope.\n",
    "    * Use $n=500$ and $n=2000$\n",
    "    * Compare the sampling speed with the previous model\n",
    "        * What can you conclude about the effects of parameter correlations in sampling speed?\n",
    "    * Can you recover the true slope?\n",
    "    * Do you get any warnings? Again explore ESS and R-hat.\n",
    "    * Also explore the correlation between model parameters. What can you conclude?\n",
    "* Try to get the flawed model in Bambi\n",
    "    * Hint: Use the formula `\"y ~ x + x\"`. \n",
    "    * What happens? Why do you think it works that way?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ib_advanced_regression')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96b52e1d985b6c457db8356caeb4e1b2f8f20b187e2d2aa9e312ebd461d4cda7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
