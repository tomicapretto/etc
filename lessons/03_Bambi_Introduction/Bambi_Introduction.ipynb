{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: The world's simplest model, now simpler\n",
    "\n",
    "* Intercept-only model in Bambi\n",
    "* Quick review of the model math\n",
    "    * Just a slide to review the concept\n",
    "* Create the model in Bambi\n",
    "    * `model = bmb.Model(\"weight ~ 1\", data)`\n",
    "    * We aren't passing priors explicitly. We'll come back to this in the next section.\n",
    "* Fit the model in Bambi\n",
    "    * `idata = model.fit()`\n",
    "    * Perhaps pass kwargs to show it\n",
    "* Explain what we just done\n",
    "    * `Model` class is our entry point to create all models in Bambi\n",
    "    * The model formula is the concise description of predictors that Bambi used\n",
    "    * The tilde \"~\" separates the response from the predictors\n",
    "    * The `1` stands for the intercept.\n",
    "        * It's as if the slope was multiplied by a constant value, which is 1.\n",
    "\n",
    "### Discussion points\n",
    "\n",
    "* Should we print the model here, and explain what are the components?\n",
    "    * This would make us explain at least a little about the priors or live with the confusion.\n",
    "    * I think that would mean two big concepts into the same section, and we don't want that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 20: What's in a Bambi model\n",
    "\n",
    "* What happens when we print a Bambi model?\n",
    "    * So much information is made available to us!\n",
    "    * Formula -> The one we entered\n",
    "    * Family -> It's a broader concept, but for now we're happy to see it says \"gaussian\".\n",
    "        * It means we're using a Gaussian likelihood function\n",
    "    * Link -> It's an important concept that will be very well covered in Lesson 4.\n",
    "        * In a nutshell, it's a transformation that is applied to the mean of the likelihood function\n",
    "        * For now we're happy knowing it says \"identity\" which means it doesn't change anything\n",
    "    * Observations: Helpful to double check!\n",
    "    * And then we have priors! \n",
    "        * Common-level effects show the priors for predictor related parameters, like the slopes or the intercept!\n",
    "        * Auxiliary parameters are all the parameters that go into the likelihood, but are not affected by predictors\n",
    "            * Like the sigma in the linear regression model!\n",
    "        * How Bambi chooses priors is another topic.\n",
    "            * But it tries to do it in such a way they are non-informative\n",
    "* And where's the PyMC model?\n",
    "    * You can access it with `model.backend.model`\n",
    "    * It's not advisable to work with it directly, but if you can do it if you just want to have some fun\n",
    "* Other cool tricks\n",
    "    * .plot_prior()\n",
    "    * .prior_predictive()\n",
    "        * I wouldn't show this one\n",
    "    * .graph()\n",
    "\n",
    "### Discussion points\n",
    "\n",
    "* Are the following two \"sections\" needed?\n",
    "    * \"And where's the PyMC model?\"\n",
    "    * \"Other cool tricks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 30: Slopes\n",
    "\n",
    "* Adding a slope is so easy\n",
    "    * Just add the name of the predictor to the model formula\n",
    "    * \"y ~ 1 + response\"\n",
    "    * Even if it has a transformation!\n",
    "        * We can use functions in the model formula, such as `np.log(x)`\n",
    "        * It's allowed for both the response and the predictors\n",
    "* Show model description, see the automatic prior\n",
    "* Fit model, see everything looks well\n",
    "    * Some visualizations as well\n",
    "\n",
    "### Discussion points\n",
    "\n",
    "* Show plot_cap?\n",
    "* I'm afraid this may be too short lesson... but is it a problem?\n",
    "* We could discuss more about tranformations here, but I'm not sure if it's the right place\n",
    "    * Again, it would put two important topics (add a slope and how tranformation works) in one section.\n",
    "    * I think it's better to have it as the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 40: Transformations in Bambi\n",
    "\n",
    "* How is it that `np.log(x)` worked?\n",
    "    * Is it because Bambi loads numpy internally?\n",
    "        * Nope. It's because Bambi allows you to use functions available in the environment where you create the Bambi model\n",
    "        * Plus, it has built-in transformations!\n",
    "* Built-in transformations\n",
    "    * The most important are\n",
    "        * Center\n",
    "        * Scale\n",
    "    * Show what they do, and when they are recommended\n",
    "* Extra message:\n",
    "    * If you are going to use centered or scaled predictors, it is better to let Bambi handle it with Center or Scale rather than doing it manually in the data frame. There are some rough edges that Bambi will make sure you avoid.\n",
    "\n",
    "### Discussion points\n",
    "\n",
    "* Is this section really necessary?\n",
    "    * I think it's a nice to have\n",
    "    * But it could be an appendix as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 50: Modeling categories\n",
    "\n",
    "* Even though species is of a different nature, we don't need to change anything to the model formula\n",
    "    * We just pass it, and Bambi knows how to handle it\n",
    "* \"y ~ 1 + species\"\n",
    "    * Why does Bambi drop the first species in species? \n",
    "        * Because the species variable would be confounded with the intercept.\n",
    "        * And that would imply the model is non-identifiable.\n",
    "        * Dropping a level is one of the many constrains one could apply to the categorical variable to make the model identifiable.\n",
    "* \"y ~ 0 + species\" \n",
    "    * But another option is dropping the intercept\n",
    "    * Since there's no intercept, there's no need to apply any restriction to the categorical predictor and Bambi retains a coefficient for every species. Just like the PyMC model we wrote in the exercises of Lesson 2.\n",
    "    * So here we're also showing the meaning of the \"0\" in the model formula.\n",
    "* Show the estimated coefficients are different under both models...\n",
    "    * Finish the section showing this, and wondering why this is happening\n",
    "    * Say this will be covered in the next section.\n",
    "\n",
    "\n",
    "### Discussion points\n",
    "\n",
    "* It would be possible to add the explanations to the different encodings here, but I think it would make the section too long and complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 60: Understanding encodings\n",
    "\n",
    "* Categorical variables need to be encoded into numbers somehow\n",
    "* A default approach is to create dummy variables\n",
    "* But since the sum(x, axis=1) adds up to a column of 1s, keeping all the dummies would introduce perfect association between predictors if we also have an intercept\n",
    "    * And for complicated reasons this is not something we want. \n",
    "    * It's going to break our inferences\n",
    "    * We say the model is not-identified\n",
    "* Different alternatives\n",
    "    * Never use an intercept\n",
    "        * It's successful only if we have one categorical predictor\n",
    "        * The meaning of the coefficient is the mean of the group\n",
    "            * This is known as cell-means encoding\n",
    "    * Use encodings that apply restrictions\n",
    "        * By default, Bambi uses Treatment encoding (or Reference encoding)\n",
    "        * One level is dropped, and that level is the Reference level of the variable\n",
    "        * The meaning of the coefficients is then the difference between the any category and the reference level\n",
    "* It's a complex topic\n",
    "    * There are many different encodingss\n",
    "    * And things get more complicated when there are more categorical terms, it's hard to make sure we're not introducing non-identifiabilities\n",
    "    * Fortunately, Bambi handles all these cases automatically for us\n",
    "    * Even though it's not completely clear for us, we can rely on Bambi :)\n",
    "    * But it's always good to try to figure out what's going on!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 70: The full model\n",
    "\n",
    "Goals: \n",
    "\n",
    "* Show how to create the model with multiple intercepts and multiple slopes\n",
    "* We need to introduce the interaction operator\n",
    "    * `numeric:categoric` gives different slopes of `numeric` for every level in `categoric`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 80: Predictions\n",
    "\n",
    "* .predict() method\n",
    "* much simpler than PyMC\n",
    "* Out-of-sample predictions is _extremely_ easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 90: End to end analysis with Bambi\n",
    "\n",
    "* Model specification\n",
    "* Explore priors\n",
    "* Explore prior predictive distribution\n",
    "* Change priors?\n",
    "* Updte model\n",
    "* Explore model again\n",
    "* Fit model\n",
    "* Explore posteriors\n",
    "* Diagnose posteriors\n",
    "* Show fitted curve\n",
    "* Predict weigth for new observations\n",
    "* ... I think this is it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To discuss\n",
    "\n",
    "* When do we show how to change the automatic priors?\n",
    "    * Is it needed in this lesson?\n",
    "        * I feel yes, but I'm not sure what's the right place\n",
    "* Predictive distributions\n",
    "    * Students should be familiar because of the introductory course.\n",
    "    * I think we _could_ show them without much explanation, but it would be better to provide one or two sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts and things I don't want to miss\n",
    "\n",
    "* Bambi\n",
    "* Automatic Priors\n",
    "* Formulae syntax\n",
    "* Prior predictive sampling in Bambi\n",
    "* Predictions in Bambi\n",
    "\n",
    "...\n",
    "\n",
    "* Bambi has automatic priors.\n",
    "  * Main points:\n",
    "    * Designed to provide low to moderate shrinkage/regularization in most situations.\n",
    "    * Allows faster prototyping and iteration.\n",
    "    * Helps you getting focused on the model and the problem. You can refine priors once your model is selected.\n",
    "* Bambi allows to pass inline transforms (eg log(y) ~ x).\n",
    "    * Compare manual transform with inline transform.\n",
    "* Show Bambi repr and how nice it summarizes everything\n",
    "\n",
    "... \n",
    "\n",
    "* At the end of this lesson this person should feel impressed with Bambi and understand\n",
    "  * The equivalent functionaly\n",
    "  * Syntatical sugar of Bambi\n",
    "  * Less code to type, fewer errors to make.\n",
    "  * Bambi allows to iterate faster when exploring different models.\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "* Bambi vs PyMC comparison (in terms of code)\n",
    "* When do we talk about power terms?\n",
    "    * They will be useful for the concrete example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
